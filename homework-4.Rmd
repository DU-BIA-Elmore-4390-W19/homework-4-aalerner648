---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Andrew Lerner"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(tree)
theme_set(theme_bw())
```


## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

 In the lab, we applied random forests to the Boston data using mtry=6
and using ntree=25 and ntree=500. Create a plot displaying the test
error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your
plot after Figure 8.10. Describe the results obtained.

## Answer 1

```{r}
set.seed(1)
df <- tbl_df(Boston)
inTraining <- createDataPartition(df$medv, p = .75, list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]

# Since this problem was indicated as ungraded I wanted to submit the work 
# I completed on it rather than copying your code from last night - I think 
# I made significant progress (and worked hard on it) despite not getting all
# the way there. If you have any feedback on the way I went about this I'd 
# love to hear, though if not no big deal as well. 

x <- data.frame(mtry = numeric(), RMSE = numeric(), tree = numeric())

for (a in training) {
  i = 0      
  rf_boston_cv <- train(medv ~ ., 
                  data = training,
                  method = "rf",
                  ntree = 25 + (i * 25),
                  importance = T,
                  tuneGrid = data.frame(mtry = 3:9))
  results <- rf_boston_cv$results[1:2]
  results$tree <- 25 + (i * 25)
  x <- rbind(x,results)
  i <- i + 1
  if(i == 19) {
    break}
}
x

```
```{r}
p <- ggplot(data = x, aes(x = x$mtry, y = x$RMSE))
p + geom_line(col = "#6E0000") + 
  theme_bw()
```



## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
2. Fit a multiple regression model to the training data and report the 
estimated test MSE
3. Summarize your results. 

Part A)
```{r}
set.seed(9823)
df <- tbl_df(Carseats)
inTraining <- createDataPartition(df$Sales, p = .50, list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]
```

Part B)

Here we can see that if we have a good shelf location we are much more likely to make sale, where consumers may also be considering price and their education level has an impact. If we have a bad shelf location, our path is more difficult - if we have a low price we still have a good probability of making a sale, but if we have a high price consumers start to consider things such as our competitors' prices.

We can also see that the test MSE is 4.48

```{r}
carseats_tree <- rpart(Sales ~ ., data = training)
summary(carseats_tree)
```

```{r}
prp(carseats_tree)
```

```{r}
yhat <- predict(carseats_tree, newdata = testing)
mean((yhat - testing$Sales)^2)
```


Part C)


```{r}
fit_control <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 10)
carseats_tree <- train(Sales ~ ., 
                        data = training,
                        method = "rpart", 
                        trControl = fit_control)
plot(carseats_tree)
```


```{r}
plot(as.party(carseats_tree$finalModel))
```


Part D)

Using the bagged approach, the test MSE is now 4.48 - an improvement over our previous model. Price and shelf location are both still the most important variables here.

```{r}
carseats_bag = randomForest(Sales ~ ., data = training, mtry = 10, ntree = 500, importance = TRUE)
carseats_bag
```


```{r}
test_preds <- predict(carseats_bag, newdata = testing)
carseats_testing2 <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(carseats_testing2$sq_err)
```


```{r}
importance(carseats_bag)
```


Part E)

The test MSE decreased to 3.61, though price and shelf location remain the most important variables involved.

```{r}
carseat_forest = randomForest(Sales ~ ., data = training, mtry = 3, ntree = 500, importance = TRUE)
carseat_forest
```

```{r}
test_preds2 <- predict(carseat_forest, newdata = testing)
carseats_test3 <- testing %>%
  mutate(y_hat = test_preds2,
         sq_err = (y_hat - Sales)^2)
mean(carseats_test3$sq_err)
```

```{r}
importance(carseat_forest)
```

Part F) 

```{r}
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)
trainControl <- trainControl(method = "cv", number = 5)
carseat_gbm <- train(Sales ~ ., 
                    data = training, 
                    distribution = "gaussian", 
                    method = "gbm",
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
carseat_gbm
```

```{r}
plot(carseat_gbm)
```

```{r}
test_preds3 <- predict(carseat_gbm, newdata = testing)
carseat_test4 <- testing %>%
  mutate(y_hat_gbm = test_preds3,
         sq_err_gbm = (y_hat_gbm - Sales)^2)
mean(carseat_test4$sq_err_gbm)
```

Part G)

```{r}
carseat_lm = lm(Sales ~ ., data = training)
summary(carseat_lm)
```

```{r}
test_preds4 <- predict(carseat_lm, newdata = testing)
carseat_test5 <- testing %>%
  mutate(y_hat_lm = test_preds4,
         sq_err_lm = (y_hat_lm - Sales)^2)
mean(carseat_test5$sq_err_lm)
```

Part H)

Summarize Results:  Despite running a variety of more complicated models, our multiple linear regression model actually displayed the lowest test MSE at 1.01 - in addition to having the highest degree of interpretability of any model. It is the clear winner. That being said, the boosted gradient also did quite well with a test MSE of 1.76 and was a significant improvement over our other models (random forest, bagged tree, etc.) Price and shelf location remained our most important variables throughout, which should result in implementation of actionable business recommendations. 

